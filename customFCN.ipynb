{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Custom Fully Convolutional Networks For Time Series Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Three Different Time Series Datasets by using sktime's `load_UCR_UEA_dataset` \n",
    "#### About the datasets:\n",
    "* First dataset is called **DistalPhalanxOutlineCorrect**. It has 2 classes, It is of type Image, and it has one dimension.\n",
    "* Second dataset is called **ItalyPowerDemand**. It has 2 classes, It is of type Sensor, and it has one dimension.\n",
    "* Third dataset is called **BME**. It has 3 classes, It is of type Simulated, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing necessary packages!\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load_UCR_UEA_dataset\n",
    "from sktime.datasets import load_UCR_UEA_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing Time Series Datasets!\n",
    "# 2 Classes, 1D, Image\n",
    "X_train_1, y_train_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_1, y_test_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 2 Classes, 1D, Sensor \n",
    "X_train_2, y_train_2 = load_UCR_UEA_dataset(name= \"ItalyPowerDemand\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_2, y_test_2 = load_UCR_UEA_dataset(name=\"ItalyPowerDemand\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 3Classes, 1D, Simulated\n",
    "X_train_3, y_train_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_3, y_test_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"test\", return_type=\"numpy2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: (600, 80) (276, 80)\n",
      "Dataset 2: (67, 24) (1029, 24)\n",
      "Dataset 3: (30, 128) (150, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1:\", X_train_1.shape, X_test_1.shape)\n",
    "print(\"Dataset 2:\", X_train_2.shape, X_test_2.shape)\n",
    "print(\"Dataset 3:\", X_train_3.shape, X_test_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different datasets, which means we must have a function that can take in different datasets with different length.  \n",
    "\n",
    "The plain baseline is a **Fully Convolutional Networks** which is build by stacking three convolution blocks with the filter sizes {128,256, 128} in each block.  \n",
    "* Unlike the MCNN and MC-CNN, We exclude any pooling operation.  \n",
    "* The basic block is a convolutional layer followed by a batch normalization layer and a ReLU activation layer.  \n",
    "* The convolution operation is fulfilled by three 1-D kernels with the sizes {8, 5, 3} without striding.\n",
    "* Batch normalization is applied to speed up the convergence speed and help improve generalization. \n",
    "* After the convolution blocks, the features are fed into a global average pooling layer instead of a fully connected layer, which largely reduces the number of weights. The final label is produced by a softmax layer\n",
    "\n",
    "\n",
    "The FCN Model is then trained with **Adam** with learning rate 0.001, ρ = 0.9, β1 = 0.9, β2 = 0.999 and e = 1e − 8!  \n",
    "\n",
    "The loss function for the model is **categorical cross entropy**! With **accuracy** as metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are having different dataset with different types and sizes!\n",
    "def create_fcn_model(input_shape, num_classes):\n",
    "    fcn_model = tf.keras.Sequential([\n",
    "        keras.layers.Conv1D(filters=128, kernel_size=8, input_shape=input_shape),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Conv1D(filters=256, kernel_size=5),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Conv1D(filters=128, kernel_size=3),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    fcn_model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.001, epsilon=1e-8),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return fcn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining **callbacks** parameters for fine tuning!  \n",
    "Defining **label encoder** for converting data to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training sets by using **train_test_split()** to split the data into random train and test subsets!  \n",
    "* test_size = 0.2! \n",
    "    * 20% of the data will be used for the valiation/test set and the remaining 80% will be used for the training set!\n",
    "* random_state=42!\n",
    "    * Ensuring that the splits that generates are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Creating and training FCN on the FIRST dataset ----------- \n",
    "X_train_FCN_1, X_val_FCN_1, y_train_FCN_1, y_val_FCN_1 = train_test_split(X_train_1, y_train_1, test_size=0.2, random_state=42)\n",
    "# --------- Creating and training FCN on the SECOND dataset ----------- \n",
    "X_train_FCN_2, X_val_FCN_2, y_train_FCN_2, y_val_FCN_2 = train_test_split(X_train_2, y_train_2, test_size=0.2, random_state=42)\n",
    "# ------------- Creating and training FCN on the THIRD dataset ------------\n",
    "X_train_FCN_3, X_val_FCN_3, y_train_FCN_3, y_val_FCN_3 = train_test_split(X_train_3, y_train_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the training sets for the models or training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- First Training set, TYPE=Image ------------\n",
    "X_train_FCN_1, X_val_FCN_1, y_train_FCN_1, y_val_FCN_1 = train_test_split(X_train_1, y_train_1, test_size=0.2, random_state=42)\n",
    "# One-hot encode the labels\n",
    "y_train_FCN_1 = tf.keras.utils.to_categorical(y_train_FCN_1, num_classes=np.unique(y_train_FCN_1).shape[0])\n",
    "y_val_FCN_1 = tf.keras.utils.to_categorical(y_val_FCN_1, num_classes=np.unique(y_train_FCN_1).shape[0])\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_FCN_1 = X_train_FCN_1.reshape(X_train_FCN_1.shape[0], X_train_FCN_1.shape[1], -1)\n",
    "X_val_FCN_1 = X_val_FCN_1.reshape(X_val_FCN_1.shape[0], X_val_FCN_1.shape[1], -1)\n",
    "\n",
    "# --------- Second Training set, TYPE=Sensor ----------- \n",
    "X_train_FCN_2, X_val_FCN_2, y_train_FCN_2, y_val_FCN_2 = train_test_split(X_train_2, y_train_2, test_size=0.2, random_state=42)\n",
    "# Converting to int.\n",
    "y_train_FCN_2 = le.fit_transform(y_train_FCN_2) #Converting to int\n",
    "y_val_FCN_2 = le.transform(y_val_FCN_2) #Converting to int\n",
    "# One-hot encode the labels\n",
    "y_train_FCN_2 = tf.keras.utils.to_categorical(y_train_FCN_2, num_classes=int(np.max(y_train_FCN_2) + 1))\n",
    "y_val_FCN_2 = tf.keras.utils.to_categorical(y_val_FCN_2, num_classes= int(np.max(y_train_FCN_2) + 1))\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_FCN_2 = X_train_FCN_2.reshape(X_train_FCN_2.shape[0], -1)\n",
    "X_val_FCN_2 = X_val_FCN_2.reshape(X_val_FCN_2.shape[0], -1)\n",
    "\n",
    "# ------------- Third Training set, TYPE=Simulated ------------- #\n",
    "X_train_FCN_3, X_val_FCN_3, y_train_FCN_3, y_val_FCN_3 = train_test_split(X_train_3, y_train_3, test_size=0.2, random_state=42)\n",
    "# Converting to int.\n",
    "y_train_FCN_3 = le.fit_transform(y_train_FCN_3) #Converting to int\n",
    "y_val_FCN_3 = le.transform(y_val_FCN_3) #Converting to int\n",
    "# Subtract 1 from the labels to make them start from 0\n",
    "y_train_FCN_3 = y_train_FCN_3 - 1\n",
    "y_val_FCN_3 = y_val_FCN_3 - 1\n",
    "# One-hot encode the labels\n",
    "y_train_FCN_3 = tf.keras.utils.to_categorical(y_train_FCN_3, num_classes=int(np.max(y_train_FCN_3) + 1))\n",
    "y_val_FCN_3 = tf.keras.utils.to_categorical(y_val_FCN_3, num_classes=int(np.max(y_train_FCN_3) + 1))\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_FCN_3 = X_train_FCN_3.reshape(X_train_FCN_3.shape[0], -1)\n",
    "X_val_FCN_3 = X_val_FCN_3.reshape(X_val_FCN_3.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing and shuffling the training sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ First Training Set! ---------------\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "train_dataset_FCN_1 = tf.data.Dataset.from_tensor_slices((X_train_FCN_1, y_train_FCN_1))\n",
    "val_dataset_FCN_1 = tf.data.Dataset.from_tensor_slices((X_val_FCN_1, y_val_FCN_1))\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "train_dataset_FCN_1 = train_dataset_FCN_1.shuffle(buffer_size=1024).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset_FCN_1 = val_dataset_FCN_1.batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# ------------ Second Training Set! ---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
