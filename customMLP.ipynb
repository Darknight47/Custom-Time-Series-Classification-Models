{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Custom Multi Layer Perceptron For Time Series Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Three Different Time Series Datasets by using sktime's `load_UCR_UEA_dataset` \n",
    "#### About the datasets:\n",
    "* First dataset is called **DistalPhalanxOutlineCorrect**. It has 2 classes, It is of type Image, and it has one dimension.\n",
    "* Second dataset is called **ItalyPowerDemand**. It has 2 classes, It is of type Sensor, and it has one dimension.\n",
    "* Third dataset is called **BME**. It has 3 classes, It is of type Simulated, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing necessary packages!\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load_UCR_UEA_dataset\n",
    "from sktime.datasets import load_UCR_UEA_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing Time Series Datasets!\n",
    "# 2 Classes, 1D, Image\n",
    "X_train_1, y_train_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_1, y_test_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 2 Classes, 1D, Sensor \n",
    "X_train_2, y_train_2 = load_UCR_UEA_dataset(name= \"ItalyPowerDemand\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_2, y_test_2 = load_UCR_UEA_dataset(name=\"ItalyPowerDemand\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 3Classes, 1D, Simulated\n",
    "X_train_3, y_train_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_3, y_test_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"test\", return_type=\"numpy2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: (600, 80) (276, 80)\n",
      "Dataset 2: (67, 24) (1029, 24)\n",
      "Dataset 3: (30, 128) (150, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1:\", X_train_1.shape, X_test_1.shape)\n",
    "print(\"Dataset 2:\", X_train_2.shape, X_test_2.shape)\n",
    "print(\"Dataset 3:\", X_train_3.shape, X_test_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different datasets, which means we must have a function that can take in different datasets with different length.  \n",
    "\n",
    "The plain baseline is a basic **Multi Layer Perceptron** which is build by stacking three fully connected layers. \n",
    "\n",
    "The fully connected layers **each has 500 neurons** following 2 designs roles:\n",
    "* Using **Dropout** at each layer's input to improve the generalization capability!  \n",
    "    * The fraction of the input units to drop on the input layer is **0.1**\n",
    "    * The fraction of the input units to drop on the second and third layers is **0.2**\n",
    "    * The fraction of the input units to drop on the output layer is **0.3**\n",
    "* Using **ReLU** as activation function to prevent saturation of the gradient when the network is deep!  \n",
    "And the output layer ends the network with a **softmax** activation function!\n",
    "\n",
    "The MLP Model is then trained with **Adadelta** with learning rate 0.1, ρ = 0.95 and e = 1e − 8!  \n",
    "\n",
    "The loss function for the model is **categorical cross entropy**! With **accuracy** as metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, num_classes):\n",
    "    mlp_model = tf.keras.Sequential([\n",
    "        keras.layers.Dropout(0.1, input_shape= input_shape),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    mlp_model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-8),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining **callbacks** parameters for fine tuning!  \n",
    "Defining **label encoder** for converting data to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numerical variables.\n",
    "le = LabelEncoder() \n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training sets by using **train_test_split()** to split the data into random train and test subsets!  \n",
    "* test_size = 0.2! \n",
    "    * 20% of the data will be used for the valiation/test set and the remaining 80% will be used for the training set!\n",
    "* random_state=42!\n",
    "    * Ensuring that the splits that generates are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Creating and training MLP on the FIRST dataset ----------- \n",
    "X_train_MLP_1, X_val_MLP_1, y_train_MLP_1, y_val_MLP_1 = train_test_split(X_train_1, y_train_1, test_size=0.2, random_state=42)\n",
    "# --------- Creating and training MLP on the SECOND dataset ----------- \n",
    "X_train_MLP_2, X_val_MLP_2, y_train_MLP_2, y_val_MLP_2 = train_test_split(X_train_2, y_train_2, test_size=0.2, random_state=42)\n",
    "# ------------- Creating and training MLP on the THIRD dataset ------------\n",
    "X_train_MLP_3, X_val_MLP_3, y_train_MLP_3, y_val_MLP_3 = train_test_split(X_train_3, y_train_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the training sets for the models or training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- First Training set, TYPE=Image ------------\n",
    "# One-hot encode the labels\n",
    "y_train_MLP_1 = tf.keras.utils.to_categorical(y_train_MLP_1, num_classes=np.unique(y_train_MLP_1).shape[0])\n",
    "y_val_MLP_1 = tf.keras.utils.to_categorical(y_val_MLP_1, num_classes=np.unique(y_train_MLP_1).shape[0])\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_MLP_1 = X_train_MLP_1.reshape(X_train_MLP_1.shape[0], -1)\n",
    "X_val_MLP_1 = X_val_MLP_1.reshape(X_val_MLP_1.shape[0], -1)\n",
    "\n",
    "# --------- Second Training set! TYPE=Sensor ------------\n",
    "# Converting to int.\n",
    "y_train_MLP_2 = le.fit_transform(y_train_MLP_2) #Converting to int\n",
    "y_val_MLP_2 = le.transform(y_val_MLP_2) #Converting to int\n",
    "# One-hot encode the labels\n",
    "y_train_MLP_2 = tf.keras.utils.to_categorical(y_train_MLP_2, num_classes=int(np.max(y_train_MLP_2) + 1))\n",
    "y_val_MLP_2 = tf.keras.utils.to_categorical(y_val_MLP_2, num_classes= int(np.max(y_train_MLP_2) + 1))\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_MLP_2 = X_train_MLP_2.reshape(X_train_MLP_2.shape[0], -1)\n",
    "X_val_MLP_2 = X_val_MLP_2.reshape(X_val_MLP_2.shape[0], -1)\n",
    "\n",
    "# ---------- Third Training set! TYPE=Simulated ------------\n",
    "# Converting to int.\n",
    "y_train_MLP_3 = le.fit_transform(y_train_MLP_3) #Converting to int\n",
    "y_val_MLP_3 = le.transform(y_val_MLP_3) #Converting to int\n",
    "# Subtract 1 from the labels to make them start from 0\n",
    "y_train_MLP_3 = y_train_MLP_3 - 1\n",
    "y_val_MLP_3 = y_val_MLP_3 - 1\n",
    "# One-hot encode the labels\n",
    "y_train_MLP_3 = tf.keras.utils.to_categorical(y_train_MLP_3, num_classes=int(np.max(y_train_MLP_3) + 1))\n",
    "y_val_MLP_3 = tf.keras.utils.to_categorical(y_val_MLP_3, num_classes=int(np.max(y_train_MLP_3) + 1))\n",
    "# Reshaping the dataset (batch_size, steps)\n",
    "X_train_MLP_3 = X_train_MLP_3.reshape(X_train_MLP_3.shape[0], -1)\n",
    "X_val_MLP_3 = X_val_MLP_3.reshape(X_val_MLP_3.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing and shuffling the training sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ First Training Set! ---------------\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "train_dataset_MLP_1 = tf.data.Dataset.from_tensor_slices((X_train_MLP_1, y_train_MLP_1))\n",
    "val_dataset_MLP_1 = tf.data.Dataset.from_tensor_slices((X_val_MLP_1, y_val_MLP_1))\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "train_dataset_MLP_1 = train_dataset_MLP_1.shuffle(buffer_size=1024).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset_MLP_1 = val_dataset_MLP_1.batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# ----------- Second Training Set! --------------\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "train_dataset_MLP_2 = tf.data.Dataset.from_tensor_slices((X_train_MLP_2, y_train_MLP_2))\n",
    "val_dataset_MLP_2 = tf.data.Dataset.from_tensor_slices((X_val_MLP_2, y_val_MLP_2))\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "train_dataset_MLP_2 = train_dataset_MLP_2.shuffle(buffer_size=1024).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset_MLP_2 = val_dataset_MLP_2.batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# ------------ Third Training Set! ---------------\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "train_dataset_MLP_3 = tf.data.Dataset.from_tensor_slices((X_train_MLP_3, y_train_MLP_3))\n",
    "val_dataset_MLP_3 = tf.data.Dataset.from_tensor_slices((X_val_MLP_3, y_val_MLP_3))\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "train_dataset_MLP_3 = train_dataset_MLP_3.shuffle(buffer_size=1024).batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset_MLP_3 = val_dataset_MLP_3.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and Fitting the custom MLP model for the first dataset!\n",
    "* Training the model on the first dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model_MLP_1 = create_mlp_model(X_train_MLP_1.shape[1:], np.unique(y_train_MLP_1).shape[0])\n",
    "# Train the model\n",
    "model_MLP_1.fit(train_dataset_MLP_1, epochs=10, validation_data=val_dataset_MLP_1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model on unseen data!\n",
    "* Pre-Processing The Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test labels are not needed because they are already one-hot encoded!\n",
    "# Applying to_categorical would add an extra dimension!\n",
    "# Reshape the test dataset\n",
    "X_test_11 = X_test_1.reshape(X_test_1.shape[0], -1)\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "test_dataset_MLP_1 = tf.data.Dataset.from_tensor_slices((X_test_11, y_test_1))\n",
    "# Batch and prefetch the dataset\n",
    "test_dataset_MLP_1 = test_dataset_MLP_1.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating the Custom MLP model on the **first** dataset!\n",
    "    * First dataset is called **DistalPhalanxOutlineCorrect**. It has 2 classes, It is of type Image, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6766 - accuracy: 0.5870\n",
      "Test accuracy: 0.5869565010070801\n",
      "Test loss: 0.6766079664230347\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_MLP_1.evaluate(test_dataset_MLP_1)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and Fitting the custom MLP model for the second dataset!\n",
    "* Training the model on the second dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model_MLP_2 = create_mlp_model(X_train_MLP_2.shape[1:], np.unique(y_train_MLP_2).shape[0])\n",
    "# Train the model\n",
    "model_MLP_2.fit(train_dataset_MLP_2, epochs=10, validation_data=val_dataset_MLP_2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model on unseen data!\n",
    "* Pre-Processing The Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to integers\n",
    "y_test_22 = le.transform(y_test_2)\n",
    "# One-hot encode the test labels\n",
    "y_test_22 = tf.keras.utils.to_categorical(y_test_22, num_classes=int(np.max(y_train_MLP_2) + 1))\n",
    "\n",
    "# Reshape the test dataset\n",
    "X_test_22 = X_test_2.reshape(X_test_2.shape[0], -1)\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "test_dataset_MLP_2 = tf.data.Dataset.from_tensor_slices((X_test_22, y_test_22))\n",
    "# Batch and prefetch the dataset\n",
    "test_dataset_MLP_2 = test_dataset_MLP_2.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating the Custom MLP model on the **second** dataset!\n",
    "    * Second dataset is called **ItalyPowerDemand**. It has 2 classes, It is of type Sensor, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6544 - accuracy: 0.5228\n",
      "Test accuracy: 0.5228376984596252\n",
      "Test loss: 0.65443354845047\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_MLP_2.evaluate(test_dataset_MLP_2)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and Fitting the custom MLP model for the third dataset!\n",
    "* Training the model on the third dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model_MLP_3 = create_mlp_model(X_train_MLP_3.shape[1:], np.unique(y_train_MLP_3).shape[0])\n",
    "# Train the model\n",
    "model_MLP_3.fit(train_dataset_MLP_3, epochs=10, validation_data=val_dataset_MLP_3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model on unseen data!\n",
    "* Pre-Processing The Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to integers and subtract 1\n",
    "y_test_33 = le.transform(y_test_3) - 1\n",
    "# One-hot encode the test labels\n",
    "y_test_33 = tf.keras.utils.to_categorical(y_test_33, num_classes=int(np.max(y_train_MLP_3) + 1))\n",
    "#  Reshape the test dataset\n",
    "X_test_33 = X_test_3.reshape(X_test_3.shape[0], -1)\n",
    "# Convert the numpy arrays to a tf.data.Dataset\n",
    "test_dataset_MLP_3 = tf.data.Dataset.from_tensor_slices((X_test_33, y_test_33))\n",
    "# Batch and prefetch the dataset\n",
    "test_dataset_MLP_3 = test_dataset_MLP_3.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating the Custom MLP model on the **third** dataset!\n",
    "    * Third dataset is called **BME**. It has 3 classes, It is of type Simulated, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6011 - accuracy: 0.5267\n",
      "Test accuracy: 0.5266666412353516\n",
      "Test loss: 0.6010997295379639\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_MLP_3.evaluate(test_dataset_MLP_3)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
