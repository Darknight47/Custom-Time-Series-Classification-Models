{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Custom Multi Layer Perceptron For Time Series Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Three Different Time Series Datasets by using sktime's `load_UCR_UEA_dataset` \n",
    "#### About the datasets:\n",
    "* First dataset is called **DistalPhalanxOutlineCorrect**. It has 2 classes, It is of type Image, and it has one dimension.\n",
    "* Second dataset is called **ItalyPowerDemand**. It has 2 classes, It is of type Sensor, and it has one dimension.\n",
    "* Third dataset is called **BME**. It has 3 classes, It is of type Simulated, and it has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing necessary packages!\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load_UCR_UEA_dataset\n",
    "from sktime.datasets import load_UCR_UEA_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importing Time Series Datasets!\n",
    "# 2 Classes, 1D, Image\n",
    "X_train_1, y_train_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_1, y_test_1 = load_UCR_UEA_dataset(name=\"DistalPhalanxOutlineCorrect\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 2 Classes, 1D, Sensor \n",
    "X_train_2, y_train_2 = load_UCR_UEA_dataset(name= \"ItalyPowerDemand\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_2, y_test_2 = load_UCR_UEA_dataset(name=\"ItalyPowerDemand\", split=\"test\", return_type=\"numpy2D\")\n",
    "\n",
    "# 3Classes, 1D, Simulated\n",
    "X_train_3, y_train_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"train\", return_type=\"numpy2D\")\n",
    "X_test_3, y_test_3 = load_UCR_UEA_dataset(name = \"BME\", split=\"test\", return_type=\"numpy2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: (600, 80) (276, 80)\n",
      "Dataset 2: (67, 24) (1029, 24)\n",
      "Dataset 3: (30, 128) (150, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1:\", X_train_1.shape, X_test_1.shape)\n",
    "print(\"Dataset 2:\", X_train_2.shape, X_test_2.shape)\n",
    "print(\"Dataset 3:\", X_train_3.shape, X_test_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different datasets, which means we must have a function that can take in different datasets with different length.  \n",
    "\n",
    "The plain baseline is a basic **Multi Layer Perceptron** which is build by stacking three fully connected layers. \n",
    "\n",
    "The fully connected layers **each has 500 neurons** following 2 designs roles:\n",
    "* Using **Dropout** at each layer's input to improve the generalization capability!  \n",
    "    * The fraction of the input units to drop on the input layer is **0.1**\n",
    "    * The fraction of the input units to drop on the second and third layers is **0.2**\n",
    "    * The fraction of the input units to drop on the output layer is **0.3**\n",
    "* Using **ReLU** as activation function to prevent saturation of the gradient when the network is deep!  \n",
    "And the output layer ends the network with a **softmax** activation function!\n",
    "\n",
    "The MLP Model is then trained with **Adadelta** with learning rate 0.1, ρ = 0.95 and e = 1e − 8!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, num_classes):\n",
    "    mlp_model = tf.keras.Sequential([\n",
    "        keras.layers.Dropout(0.1, input_shape= input_shape),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    mlp_model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-8),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining **callbacks** parameters for fine tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training sets by using **train_test_split()** to split the data into random train and test subsets!  \n",
    "* test_size = 0.2! \n",
    "    * 20% of the data will be used for the valiation/test set and the remaining 80% will be used for the training set!\n",
    "* random_state=42!\n",
    "    * Ensuring that the splits that generates are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Creating and training MLP on the FIRST dataset ----------- \n",
    "X_train_MLP_1, X_val_MLP_1, y_train_MLP_1, y_val_MLP_1 = train_test_split(X_train_1, y_train_1, test_size=0.2, random_state=42)\n",
    "# --------- Creating and training MLP on the SECOND dataset ----------- \n",
    "X_train_MLP_2, X_val_MLP_2, y_train_MLP_2, y_val_MLP_2 = train_test_split(X_train_2, y_train_2, test_size=0.2, random_state=42)\n",
    "# ------------- Creating and training MLP on the Third dataset ------------\n",
    "X_train_MLP_3, X_val_MLP_3, y_train_MLP_3, y_val_MLP_3 = train_test_split(X_train_3, y_train_3, test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
